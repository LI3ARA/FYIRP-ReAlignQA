{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LI3ARA/FYIRP/blob/main/Notebooks/25/VQA_RAG/Googl_colab/host_llm_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working code"
      ],
      "metadata": {
        "id": "m3Mt_D5cFL6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install flask llama-cpp-python --quiet\n",
        "!wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok.tgz\n",
        "!chmod +x ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jngk4eW0FOrL",
        "outputId": "e6eaca24-8e42-44b6-8345-4f8e9c69d2fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your ngrok auth token (looks like 2N...)\n",
        "!./ngrok config add-authtoken 2vLvftpzesWBGHU32SAnWOWmryA_CvhQGZ7cyfpk6TEkPG1b"
      ],
      "metadata": {
        "id": "eU1U-B1kNIEF",
        "outputId": "5e22b016-a43e-4c15-f8ca-1b3f4125222d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time, requests\n",
        "from llama_cpp import Llama\n",
        "from flask import Flask, request, jsonify"
      ],
      "metadata": {
        "id": "W-J7KHGGNRB6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start ngrok on port 5000 and fetch public URL\n",
        "\n",
        "ngrok_proc = subprocess.Popen(['./ngrok', 'http', '5000'])\n",
        "time.sleep(5)\n",
        "try:\n",
        "    res = requests.get('http://localhost:4040/api/tunnels')\n",
        "    url = res.json()['tunnels'][0]['public_url']\n",
        "    print('‚úÖ Public URL:', url)\n",
        "except Exception as e:\n",
        "    print('‚ùå Could not get ngrok URL:', e)"
      ],
      "metadata": {
        "id": "23Q1cYiuMW84",
        "outputId": "f3a51257-fe64-4175-99ad-7acf14392814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Public URL: https://0232-34-16-171-130.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id='TheBloke/Llama-2-13B-chat-GGUF',\n",
        "    filename='llama-2-13b-chat.Q3_K_S.gguf',\n",
        "    n_gpu_layers=40,\n",
        "    n_ctx=2048\n",
        ")"
      ],
      "metadata": {
        "id": "8adlw1oINOkW",
        "outputId": "9d0940fa-c357-419d-c32e-e19caa7680aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8c072a489f2347c69b10cf2952de5a6e",
            "1810a08aadb540aabba47a1f92acd2fc",
            "be82ef8f8b0c41118f79bc00c858598e",
            "200e9bee33de444a922ee749c9a6c5db",
            "2d4b76d82be54f7e980ba04dabcc9a3c",
            "a2a85f91c7c249edb4f64fab09272f49",
            "31c88a5585ad49778453703229c72eee",
            "82fc804174714681a6ef470015cabc7d",
            "be67c35135e341b8a46fef422e270530",
            "dbea87103d1b4171a522257bef479930",
            "2110b2f6a80b47838f8ff201987bd59f"
          ]
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-13b-chat.Q3_K_S.gguf:   0%|          | 0.00/5.66G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c072a489f2347c69b10cf2952de5a6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/./llama-2-13b-chat.Q3_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q3_K:  281 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q3_K - Small\n",
            "print_info: file size   = 5.27 GiB (3.48 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 40\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 5120\n",
            "print_info: n_embd_v_gqa     = 5120\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 13B\n",
            "print_info: model params     = 13.02 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: layer  37 assigned to device CPU\n",
            "load_tensors: layer  38 assigned to device CPU\n",
            "load_tensors: layer  39 assigned to device CPU\n",
            "load_tensors: layer  40 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q3_K) (and 362 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  5396.11 MiB\n",
            "...................................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\n",
            "llama_init_from_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   204.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Flask app with LLaMA endpoint\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '‚úÖ LLaMA Flask API is running! Use /generate'\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    prompt = request.json.get('prompt', '')\n",
        "    if not prompt:\n",
        "        return jsonify({'error': 'Prompt is required'}), 400\n",
        "    output = llm(prompt, max_tokens=200)\n",
        "    return jsonify({'response': output['choices'][0]['text'].strip()})\n",
        "\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "9tcj_IcOMken",
        "outputId": "209c65ac-0a46-4bc8-9e9a-e3360b07580e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =     602.16 ms /     4 tokens (  150.54 ms per token,     6.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     603.06 ms /     5 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:11:07] \"POST /generate HTTP/1.1\" 200 -\n",
            "Llama.generate: 3 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:12] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:13] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:19] \"\u001b[31m\u001b[1mGET /generate HTTP/1.1\u001b[0m\" 405 -\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =   44834.83 ms /   200 runs   (  224.17 ms per token,     4.46 tokens per second)\n",
            "llama_perf_context_print:       total time =   44957.55 ms /   201 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:31] \"POST /generate HTTP/1.1\" 200 -\n",
            "Llama.generate: 2 prefix-match hit, remaining 2 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =     317.11 ms /     2 tokens (  158.55 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =   44713.26 ms /   199 runs   (  224.69 ms per token,     4.45 tokens per second)\n",
            "llama_perf_context_print:       total time =   45153.57 ms /   201 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:14:16] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Rp0AL-wF8Svw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step by step working- test code"
      ],
      "metadata": {
        "id": "1DvZGmwsFPUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without llm , just using a test text to pass"
      ],
      "metadata": {
        "id": "m-nmRAx4FUxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In colab"
      ],
      "metadata": {
        "id": "A-BhImDgFaW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqAiqrFm_gn4",
        "outputId": "0e72bfe3-b52a-4f10-e8d0-f894b888088a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ngrok in /usr/local/lib/python3.11/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok.tgz\n",
        "!chmod +x ngrok\n",
        "\n",
        "# üß† Your ngrok auth token goes here (not your API key)\n",
        "!./ngrok config add-authtoken <here>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmr8FWlpAlHR",
        "outputId": "9acb59c2-f4d4-4292-a66f-37ee2eaa149c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama_cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RMtckAoTEZSu",
        "outputId": "2d5b1431-4ff4-42f9-cb01-f7c014d95824"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement llama_cpp (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for llama_cpp\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "CxBHSwfaEYWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time, requests\n",
        "from flask import Flask\n",
        "\n",
        "# Start ngrok on port 5000\n",
        "ngrok_proc = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Try to get the public tunnel URL\n",
        "try:\n",
        "    tunnels = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnels[\"tunnels\"][0][\"public_url\"]\n",
        "    print(\"‚úÖ ngrok public URL:\", public_url)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå ngrok failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f6Mpll0_frA",
        "outputId": "98b5c725-ac53-4025-c213-4dc70f24395e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ngrok public URL: https://d67b-104-199-189-124.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"‚úÖ LLaMA Flask API is running!\"\n",
        "\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmwkYtOnA5Ke",
        "outputId": "ecc0e471-f557-4293-9394-dc66afd53381"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if ngrok is alive\n",
        "import subprocess, time, requests\n",
        "\n",
        "# Start ngrok (it runs in the background)\n",
        "ngrok_proc = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(4)\n",
        "\n",
        "# Try to read the tunnel info\n",
        "try:\n",
        "    res = requests.get(\"http://localhost:4040/api/tunnels\")\n",
        "    print(\"üîç Raw response:\", res.text)\n",
        "    data = res.json()\n",
        "    print(\"‚úÖ Public URL:\", data[\"tunnels\"][0][\"public_url\"])\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Could not fetch ngrok tunnel:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsySGo50CHKs",
        "outputId": "cbca4b9e-7d17-4e60-f3ea-51da30d3c975"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Raw response: {\"tunnels\":[{\"name\":\"command_line\",\"ID\":\"e2a1c3a7137a897877e63bb2f6a64125\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://f4cf-104-199-189-124.ngrok-free.app\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:5000\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n",
            "\n",
            "‚úÖ Public URL: https://f4cf-104-199-189-124.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "requests.get(\"http://localhost:4040/api/tunnels\").json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2daf9o11Cimp",
        "outputId": "ceff1232-2a55-4bdc-8d00-bd47ff15709e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tunnels': [{'name': 'command_line',\n",
              "   'ID': 'e2a1c3a7137a897877e63bb2f6a64125',\n",
              "   'uri': '/api/tunnels/command_line',\n",
              "   'public_url': 'https://f4cf-104-199-189-124.ngrok-free.app',\n",
              "   'proto': 'https',\n",
              "   'config': {'addr': 'http://localhost:5000', 'inspect': True},\n",
              "   'metrics': {'conns': {'count': 0,\n",
              "     'gauge': 0,\n",
              "     'rate1': 0,\n",
              "     'rate5': 0,\n",
              "     'rate15': 0,\n",
              "     'p50': 0,\n",
              "     'p90': 0,\n",
              "     'p95': 0,\n",
              "     'p99': 0},\n",
              "    'http': {'count': 0,\n",
              "     'rate1': 0,\n",
              "     'rate5': 0,\n",
              "     'rate15': 0,\n",
              "     'p50': 0,\n",
              "     'p90': 0,\n",
              "     'p95': 0,\n",
              "     'p99': 0}}}],\n",
              " 'uri': '/api/tunnels'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"‚úÖ Hello from Flask with ngrok!\"\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    data = request.get_json()\n",
        "    prompt = data.get(\"prompt\", \"No prompt provided.\")\n",
        "    return jsonify({\n",
        "        \"response\": f\"üß™ Test response to: '{prompt}'\"\n",
        "    })\n",
        "\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmUCNhATA518",
        "outputId": "987e1af2-34f7-4829-9405-c04c02726cc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:23:43] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:23:44] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:24:24] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:29:44] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In VS code"
      ],
      "metadata": {
        "id": "USnGfbJ_Fdxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In vs code\n",
        "import requests\n",
        "\n",
        "url = \"https://f4cf-104-199-189-124.ngrok-free.app/generate\"  # Replace with your real one\n",
        "response = requests.post(url, json={\"prompt\": \"Hello world!\"})\n",
        "print(\"üß† Response:\", response.json()[\"response\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "7-MlibVFFjFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "E8cvx9qlBQM2"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c072a489f2347c69b10cf2952de5a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1810a08aadb540aabba47a1f92acd2fc",
              "IPY_MODEL_be82ef8f8b0c41118f79bc00c858598e",
              "IPY_MODEL_200e9bee33de444a922ee749c9a6c5db"
            ],
            "layout": "IPY_MODEL_2d4b76d82be54f7e980ba04dabcc9a3c"
          }
        },
        "1810a08aadb540aabba47a1f92acd2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a85f91c7c249edb4f64fab09272f49",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_31c88a5585ad49778453703229c72eee",
            "value": "llama-2-13b-chat.Q3_K_S.gguf:‚Äá100%"
          }
        },
        "be82ef8f8b0c41118f79bc00c858598e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82fc804174714681a6ef470015cabc7d",
            "max": 5658980224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be67c35135e341b8a46fef422e270530",
            "value": 5658980224
          }
        },
        "200e9bee33de444a922ee749c9a6c5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbea87103d1b4171a522257bef479930",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2110b2f6a80b47838f8ff201987bd59f",
            "value": "‚Äá5.66G/5.66G‚Äá[00:32&lt;00:00,‚Äá191MB/s]"
          }
        },
        "2d4b76d82be54f7e980ba04dabcc9a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a85f91c7c249edb4f64fab09272f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c88a5585ad49778453703229c72eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82fc804174714681a6ef470015cabc7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be67c35135e341b8a46fef422e270530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbea87103d1b4171a522257bef479930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2110b2f6a80b47838f8ff201987bd59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}