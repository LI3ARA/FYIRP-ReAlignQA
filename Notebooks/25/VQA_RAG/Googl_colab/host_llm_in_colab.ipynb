{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LI3ARA/FYIRP/blob/main/Notebooks/25/VQA_RAG/Googl_colab/host_llm_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3Mt_D5cFL6S"
      },
      "source": [
        "# Working code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colab:\n",
        "\n",
        "- Run llava-cli in a Flask or FastAPI app\n",
        "\n",
        "- Expose it using ngrok so VS Code can reach it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Steps: \n",
        "- Load the LLaVA model using llava.cpp\n",
        "\n",
        "- Start a Flask server in Colab\n",
        "\n",
        "- Expose the server via ngrok\n",
        "\n",
        "- Accept an image + prompt, run llava-cli, and return the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jngk4eW0FOrL",
        "outputId": "e6eaca24-8e42-44b6-8345-4f8e9c69d2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install flask llama-cpp-python --quiet\n",
        "!wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok.tgz\n",
        "!chmod +x ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU1U-B1kNIEF",
        "outputId": "5e22b016-a43e-4c15-f8ca-1b3f4125222d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Add your ngrok auth token (looks like 2N...)\n",
        "!./ngrok config add-authtoken 2vLvftpzesWBGHU32SAnWOWmryA_CvhQGZ7cyfpk6TEkPG1b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W-J7KHGGNRB6"
      },
      "outputs": [],
      "source": [
        "import subprocess, time, requests\n",
        "from llama_cpp import Llama\n",
        "from flask import Flask, request, jsonify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Q1cYiuMW84",
        "outputId": "f3a51257-fe64-4175-99ad-7acf14392814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Public URL: https://0232-34-16-171-130.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Start ngrok on port 5000 and fetch public URL\n",
        "\n",
        "ngrok_proc = subprocess.Popen(['./ngrok', 'http', '5000'])\n",
        "time.sleep(5)\n",
        "try:\n",
        "    res = requests.get('http://localhost:4040/api/tunnels')\n",
        "    url = res.json()['tunnels'][0]['public_url']\n",
        "    print('‚úÖ Public URL:', url)\n",
        "except Exception as e:\n",
        "    print('‚ùå Could not get ngrok URL:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8c072a489f2347c69b10cf2952de5a6e",
            "1810a08aadb540aabba47a1f92acd2fc",
            "be82ef8f8b0c41118f79bc00c858598e",
            "200e9bee33de444a922ee749c9a6c5db",
            "2d4b76d82be54f7e980ba04dabcc9a3c",
            "a2a85f91c7c249edb4f64fab09272f49",
            "31c88a5585ad49778453703229c72eee",
            "82fc804174714681a6ef470015cabc7d",
            "be67c35135e341b8a46fef422e270530",
            "dbea87103d1b4171a522257bef479930",
            "2110b2f6a80b47838f8ff201987bd59f"
          ]
        },
        "id": "8adlw1oINOkW",
        "outputId": "9d0940fa-c357-419d-c32e-e19caa7680aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c072a489f2347c69b10cf2952de5a6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llama-2-13b-chat.Q3_K_S.gguf:   0%|          | 0.00/5.66G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/./llama-2-13b-chat.Q3_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q3_K:  281 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q3_K - Small\n",
            "print_info: file size   = 5.27 GiB (3.48 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 40\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 5120\n",
            "print_info: n_embd_v_gqa     = 5120\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 13B\n",
            "print_info: model params     = 13.02 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: layer  37 assigned to device CPU\n",
            "load_tensors: layer  38 assigned to device CPU\n",
            "load_tensors: layer  39 assigned to device CPU\n",
            "load_tensors: layer  40 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q3_K) (and 362 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  5396.11 MiB\n",
            "...................................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\n",
            "llama_init_from_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   204.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id='TheBloke/Llama-2-13B-chat-GGUF',\n",
        "    filename='llama-2-13b-chat.Q3_K_S.gguf',\n",
        "    n_gpu_layers=40,\n",
        "    n_ctx=2048\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tcj_IcOMken",
        "outputId": "209c65ac-0a46-4bc8-9e9a-e3360b07580e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =     602.16 ms /     4 tokens (  150.54 ms per token,     6.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     603.06 ms /     5 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:11:07] \"POST /generate HTTP/1.1\" 200 -\n",
            "Llama.generate: 3 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:12] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:13] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:19] \"\u001b[31m\u001b[1mGET /generate HTTP/1.1\u001b[0m\" 405 -\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =   44834.83 ms /   200 runs   (  224.17 ms per token,     4.46 tokens per second)\n",
            "llama_perf_context_print:       total time =   44957.55 ms /   201 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:12:31] \"POST /generate HTTP/1.1\" 200 -\n",
            "Llama.generate: 2 prefix-match hit, remaining 2 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     602.34 ms\n",
            "llama_perf_context_print: prompt eval time =     317.11 ms /     2 tokens (  158.55 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =   44713.26 ms /   199 runs   (  224.69 ms per token,     4.45 tokens per second)\n",
            "llama_perf_context_print:       total time =   45153.57 ms /   201 tokens\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 13:14:16] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Start Flask app with LLaMA endpoint\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '‚úÖ LLaMA Flask API is running! Use /generate'\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    prompt = request.json.get('prompt', '')\n",
        "    if not prompt:\n",
        "        return jsonify({'error': 'Prompt is required'}), 400\n",
        "    output = llm(prompt, max_tokens=200)\n",
        "    return jsonify({'response': output['choices'][0]['text'].strip()})\n",
        "\n",
        "app.run(port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp0AL-wF8Svw"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DvZGmwsFPUF"
      },
      "source": [
        "# Step by step working- test code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-nmRAx4FUxf"
      },
      "source": [
        "Without llm , just using a test text to pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-BhImDgFaW3"
      },
      "source": [
        "## In colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqAiqrFm_gn4",
        "outputId": "0e72bfe3-b52a-4f10-e8d0-f894b888088a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ngrok in /usr/local/lib/python3.11/dist-packages (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmr8FWlpAlHR",
        "outputId": "9acb59c2-f4d4-4292-a66f-37ee2eaa149c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xzf ngrok.tgz\n",
        "!chmod +x ngrok\n",
        "\n",
        "# üß† Your ngrok auth token goes here (not your API key)\n",
        "!./ngrok config add-authtoken <here>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RMtckAoTEZSu",
        "outputId": "2d5b1431-4ff4-42f9-cb01-f7c014d95824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement llama_cpp (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for llama_cpp\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install llama_cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxBHSwfaEYWH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f6Mpll0_frA",
        "outputId": "98b5c725-ac53-4025-c213-4dc70f24395e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ngrok public URL: https://d67b-104-199-189-124.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import subprocess, time, requests\n",
        "from flask import Flask\n",
        "\n",
        "# Start ngrok on port 5000\n",
        "ngrok_proc = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Try to get the public tunnel URL\n",
        "try:\n",
        "    tunnels = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnels[\"tunnels\"][0][\"public_url\"]\n",
        "    print(\"‚úÖ ngrok public URL:\", public_url)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå ngrok failed:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmwkYtOnA5Ke",
        "outputId": "ecc0e471-f557-4293-9394-dc66afd53381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"‚úÖ LLaMA Flask API is running!\"\n",
        "\n",
        "app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsySGo50CHKs",
        "outputId": "cbca4b9e-7d17-4e60-f3ea-51da30d3c975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Raw response: {\"tunnels\":[{\"name\":\"command_line\",\"ID\":\"e2a1c3a7137a897877e63bb2f6a64125\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://f4cf-104-199-189-124.ngrok-free.app\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:5000\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n",
            "\n",
            "‚úÖ Public URL: https://f4cf-104-199-189-124.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Check if ngrok is alive\n",
        "import subprocess, time, requests\n",
        "\n",
        "# Start ngrok (it runs in the background)\n",
        "ngrok_proc = subprocess.Popen([\"./ngrok\", \"http\", \"5000\"])\n",
        "time.sleep(4)\n",
        "\n",
        "# Try to read the tunnel info\n",
        "try:\n",
        "    res = requests.get(\"http://localhost:4040/api/tunnels\")\n",
        "    print(\"üîç Raw response:\", res.text)\n",
        "    data = res.json()\n",
        "    print(\"‚úÖ Public URL:\", data[\"tunnels\"][0][\"public_url\"])\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Could not fetch ngrok tunnel:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2daf9o11Cimp",
        "outputId": "ceff1232-2a55-4bdc-8d00-bd47ff15709e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tunnels': [{'name': 'command_line',\n",
              "   'ID': 'e2a1c3a7137a897877e63bb2f6a64125',\n",
              "   'uri': '/api/tunnels/command_line',\n",
              "   'public_url': 'https://f4cf-104-199-189-124.ngrok-free.app',\n",
              "   'proto': 'https',\n",
              "   'config': {'addr': 'http://localhost:5000', 'inspect': True},\n",
              "   'metrics': {'conns': {'count': 0,\n",
              "     'gauge': 0,\n",
              "     'rate1': 0,\n",
              "     'rate5': 0,\n",
              "     'rate15': 0,\n",
              "     'p50': 0,\n",
              "     'p90': 0,\n",
              "     'p95': 0,\n",
              "     'p99': 0},\n",
              "    'http': {'count': 0,\n",
              "     'rate1': 0,\n",
              "     'rate5': 0,\n",
              "     'rate15': 0,\n",
              "     'p50': 0,\n",
              "     'p90': 0,\n",
              "     'p95': 0,\n",
              "     'p99': 0}}}],\n",
              " 'uri': '/api/tunnels'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "requests.get(\"http://localhost:4040/api/tunnels\").json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmUCNhATA518",
        "outputId": "987e1af2-34f7-4829-9405-c04c02726cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:23:43] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:23:44] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:24:24] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Apr/2025 12:29:44] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"‚úÖ Hello from Flask with ngrok!\"\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    data = request.get_json()\n",
        "    prompt = data.get(\"prompt\", \"No prompt provided.\")\n",
        "    return jsonify({\n",
        "        \"response\": f\"üß™ Test response to: '{prompt}'\"\n",
        "    })\n",
        "\n",
        "app.run(port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USnGfbJ_Fdxn"
      },
      "source": [
        "## In VS code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7-MlibVFFjFu"
      },
      "outputs": [
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\requests\\models.py:974\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://f4cf-104-199-189-124.ngrok-free.app/generate\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your real one\u001b[39;00m\n\u001b[32m      5\u001b[39m response = requests.post(url, json={\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHello world!\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß† Response:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\requests\\models.py:978\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
            "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ],
      "source": [
        "# In vs code\n",
        "import requests\n",
        "\n",
        "url = \"https://f4cf-104-199-189-124.ngrok-free.app/generate\"  # Replace with your real one\n",
        "response = requests.post(url, json={\"prompt\": \"Hello world!\"})\n",
        "print(\"üß† Response:\", response.json()[\"response\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8cvx9qlBQM2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\lisara\\anaconda3\\envs\\irpenv\\lib\\site-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n"
          ]
        }
      ],
      "source": [
        "! pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                    \r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t=2025-04-16T10:17:29+0530 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "t=2025-04-16T10:17:29+0530 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "t=2025-04-16T10:17:29+0530 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "t=2025-04-16T10:17:29+0530 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPyngrokNgrokError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use ngrok to expose your Flask server\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyngrok\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ngrok\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m public_url = \u001b[43mngrok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPublic URL:\u001b[39m\u001b[33m\"\u001b[39m, public_url)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\ngrok.py:346\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(addr, proto, name, pyngrok_config, **options)\u001b[39m\n\u001b[32m    342\u001b[39m _upgrade_legacy_params(pyngrok_config, options)\n\u001b[32m    344\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpening tunnel named: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m api_url = \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m.api_url\n\u001b[32m    348\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    350\u001b[39m tunnel = NgrokTunnel(api_request(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/tunnels\u001b[39m\u001b[33m\"\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, data=options,\n\u001b[32m    351\u001b[39m                                  timeout=pyngrok_config.request_timeout),\n\u001b[32m    352\u001b[39m                      pyngrok_config, api_url)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\ngrok.py:171\u001b[39m, in \u001b[36mget_ngrok_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n\u001b[32m    167\u001b[39m     pyngrok_config = conf.get_default()\n\u001b[32m    169\u001b[39m install_ngrok(pyngrok_config)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\process.py:265\u001b[39m, in \u001b[36mget_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config.ngrok_path):\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config.ngrok_path]\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lisara\\anaconda3\\envs\\IRPenv\\Lib\\site-packages\\pyngrok\\process.py:428\u001b[39m, in \u001b[36m_start_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n\u001b[32m    425\u001b[39m kill_process(pyngrok_config.ngrok_path)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngrok_process.startup_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process.startup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    429\u001b[39m                             ngrok_process.logs,\n\u001b[32m    430\u001b[39m                             ngrok_process.startup_error)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[33m\"\u001b[39m\u001b[33mThe ngrok process was unable to start.\u001b[39m\u001b[33m\"\u001b[39m, ngrok_process.logs)\n",
            "\u001b[31mPyngrokNgrokError\u001b[39m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ],
      "source": [
        "# Use ngrok to expose your Flask server\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Public URL:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IRPenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1810a08aadb540aabba47a1f92acd2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a85f91c7c249edb4f64fab09272f49",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_31c88a5585ad49778453703229c72eee",
            "value": "llama-2-13b-chat.Q3_K_S.gguf:‚Äá100%"
          }
        },
        "200e9bee33de444a922ee749c9a6c5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbea87103d1b4171a522257bef479930",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2110b2f6a80b47838f8ff201987bd59f",
            "value": "‚Äá5.66G/5.66G‚Äá[00:32&lt;00:00,‚Äá191MB/s]"
          }
        },
        "2110b2f6a80b47838f8ff201987bd59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d4b76d82be54f7e980ba04dabcc9a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c88a5585ad49778453703229c72eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82fc804174714681a6ef470015cabc7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c072a489f2347c69b10cf2952de5a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1810a08aadb540aabba47a1f92acd2fc",
              "IPY_MODEL_be82ef8f8b0c41118f79bc00c858598e",
              "IPY_MODEL_200e9bee33de444a922ee749c9a6c5db"
            ],
            "layout": "IPY_MODEL_2d4b76d82be54f7e980ba04dabcc9a3c"
          }
        },
        "a2a85f91c7c249edb4f64fab09272f49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be67c35135e341b8a46fef422e270530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be82ef8f8b0c41118f79bc00c858598e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82fc804174714681a6ef470015cabc7d",
            "max": 5658980224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be67c35135e341b8a46fef422e270530",
            "value": 5658980224
          }
        },
        "dbea87103d1b4171a522257bef479930": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
